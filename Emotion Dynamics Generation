#This code shows how our data linkage process (i.e. final_generation) was adapted to provide the relevant emotion dynamics metrics in Chapter 6. 
#As an example, we have shown how these metrics are generated for LIWC PosEmo

def time_dataframe(data_f, id):
    if id in data_f['linker'].unique():
        is_id = data_f['linker'] == id
        df_filter = data_f[is_id] # Filters dataset by row

        time_counts = df_filter.tweet_hour.value_counts() # Generates count of each time window

        keys = np.array(time_counts.index)
        value = np.array(time_counts.values)
        diction = {keys[i]: value[i] for i in range(len(keys))} # Generates dictionary from time window data

        missing_times(diction, keys) # If dictionary key is missing (i.e. no Tweets during this time), add key with value 0

        df_times = pd.DataFrame.from_dict(diction, orient='index')
        df_times = df_times.transpose() # Create dataframe from this dictionary

        df_times['linker'] = id # Add ID column to dataframe
        return df_times
    
def missing_times(dic, keys):
    for n in range(0,24):
        if n not in keys:
            dic.update({n: 0})
    return dic

id_list = np.array(td['linker'].unique())

def data_gen(data_f, out_name):
    id_list = np.array(data_f['linker'].unique())
    frames = []
    
    for item in id_list:
        dt = time_dataframe(data_f, item)
        frames.append(dt)

    result = pd.concat(frames)

    var_name = out_name

    str = var_name

    globals()[str] = result

    return var_name

def final_generation(df, name):
    
    data_gen(df, 'final_df')
    global final_df
    final_df = final_df[['linker',0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]]
    # Generating total tweets variables
    final_df['total_tweets'] = final_df[0] + final_df[1] + final_df[2] + final_df[3] + final_df[4] + final_df[5] + final_df[6] + final_df[7] + final_df[8] + + final_df[9] + + final_df[10] + final_df[11]+ final_df[12]+ final_df[13]+ final_df[14]+ final_df[15]+ final_df[16]+ final_df[17]+ final_df[18]+ final_df[19]+ final_df[20]+ final_df[21] + final_df[22] + final_df[23]  

    #Generating row_num column, referring the row number of each of the dataframe's row
    final_df['row_num'] = np.arange(len(final_df))
    final_df['row_num']
    
    #Merging participant level Twitter dataset to ALSPAC survey dataset

    link = final_df[['linker', 'total_tweets']]
    final_d = surv.merge(final_df)
    final_d = final_d.sort_values(by=['row_num'])
    
    df['epicosmliwcposemo'] = df['epicosmliwcposemo'].fillna(0)
    pos_df = df.groupby('linker')['epicosmliwcposemo'].mean().reset_index()
    pos_df.rename(columns = {'epicosmliwcposemo': 'Mean Posemo Score'}, inplace=True)
    final_d = pd.merge(final_d, pos_df)
    
    poss_df = df.groupby('linker')['epicosmliwcposemo'].std().reset_index()
    poss_df.rename(columns = {'epicosmliwcposemo': 'Std Posemo Score'}, inplace=True)
    final_d = pd.merge(final_d, poss_df)

    final_d['aln'] = final_d['aln'].astype(str)
    final_d['linker'] = final_d['aln'] + ' ' + final_d['qlet']
    
    final_d['generation'] = None
    final_d.loc[(final_d['qlet'] == 'A') | (final_d['qlet'] == 'B'), 'generation'] = 'G1'
    final_d.loc[(final_d['qlet'] == 'P') | (final_d['qlet'] == 'M'), 'generation'] = 'G0'

    print(final_d)

    final_d.to_csv(f'C:\\Users\\ta20395\\TD\\Second Harvest Data\\{name}.csv')

final_generation(tl_df, 'final_df')

#Prior to running the MSSD code below, you must generate a variable for the date each Tweet was posted at.
#This should be in the format 2019-12-04 17:00:00. 

no_gap = []
def calculate_mssd(df, sentiment):
    
    # Sort the DataFrame by date Tweet posted
    df = df.sort_values(by='date')
    
    df = df.groupby('date', as_index=False)[sentiment].mean()

    # Calculate the time differences between consecutive rows
    df['time_diff'] = df['date'].diff()

    # Convert this to a float value - e.g. a differences of 10 days and 12 hours is a float value of 10.5
    df['time_diff_float'] = df['time_diff'].dt.days * 24 + df['time_diff'].dt.seconds / 3600
    gap = len(df.loc[df['time_diff_float'] == 0.0])
    no_gap.append(gap)

    # Calculate the squared differences between consecutive sentiments
    df['consecutive_sentiments_diff'] = ((df[sentiment] - df[sentiment].shift(1))**2) / df['time_diff_float']

    # Calculate the time-adjusted MSSD measure
    median_time_diff = df['time_diff_float'].median()
    mssd_time_adjusted = df['consecutive_sentiments_diff'].sum() * (median_time_diff / (len(df) - 1))
    return mssd_time_adjusted

def apply_mssd(var, col_name, list_sent, data):
    for participant_id in data['linker'].unique():
        participant_data = data[data['linker'] == participant_id]
        mssd = calculate_mssd(participant_data, var)
        list_sent.append({'linker': participant_id, col_name: mssd})

def mssd_to_df(data, Q):
    # Identify rows where 'Column2' appears more than once
    duplicates_mask = data.duplicated(subset='linker', keep=False)
    # Keep only rows where 'Column2' appears more than once
    data = data[duplicates_mask]
    if len(data) < 0:
        print("No valid Tweets")
    else:
        mssd_scores_posemo = []
        apply_mssd('epicosmliwcposemo', 'Posemo MSSD', mssd_scores_posemo, data)
        df = pd.DataFrame(mssd_scores_posemo)
        df['Q'] = Q
        return df

mssd = mssd_to_df(tl_df, '1')

pd.merge(final_df, mssd)
