# -----------------------------------------------
# Linkage of ALSPAC survey data to Twitter dataset
# -----------------------------------------------
# This script combines ALSPAC participant survey responses with aggregated Twitter behaviour,
# resulting in a participant-level dataset useful for analysis.
# Input dataframes:
# - surv_data: ALSPAC survey data
# - tweet_data: Summarised Twitter metrics per participant
#
# Notes:
# - ALSPAC variable names have been replaced to preserve confidentiality.
# - Initial linkage (cleaning, full ALSPAC dataset generation) is described below, but not included 
#   due to use of protected variables.
# -----------------------------------------------

# Step 1: Import required packages
import pandas as pd
import numpy as np

# -----------------------------------------------
# Data Cleaning Overview (Prior to linkage)
# -----------------------------------------------
# Recommended pre-processing steps (not shown in code):
# 1. Exclude participants from tweet_data who posted zero Tweets.
# 2. In both surv_data and tweet_data, create a unique participant identifier ("linker") using
#    family code and generation code variables.
# -----------------------------------------------

# Step 2: Generating Participant-Level ALSPAC Data
# -----------------------------------------------
# The initial survey data (surv_data) is row-wise by family.
# To analyse at the participant level:
# - Create 3 copies of surv_data (one per participant type: mother, partner, G1 child).
# - Apply ALSPAC eligibility filters within each dataset.
# - Add a label variable marking participant type ('M' for Mother, etc).
# - Concatenate all into a single participant-level dataframe.
# -----------------------------------------------

# Step 3: Restrict survey data to participants present in Twitter data
sd_link = list(surv_data['linker'])
td_link = list(tweet_data['linker'])
common_link = set(sd_link).intersection(td_link)
len(common_link)  # Returns the count of eligible participants

# Restrict to relevant rows
surv_filt = surv_data[surv_data['linker'].isin(td_link)]

# -----------------------------------------------
# Step 4: Merge and Aggregate Tweet Data
# -----------------------------------------------
# Merge filtered survey data with twitter metrics:
tl_df = tweet_data.merge(surv_filt)

# tl_df now contains, for each participant, both survey responses and Tweet data.
# Further filtering (by date, Tweet type, Tweet content) can be performed on tl_df as needed.
# -----------------------------------------------

# -----------------------------------------------
#  Linkage Functions
# -----------------------------------------------

def time_dataframe(data_f, id):
    """
    Builds a dataframe summarising Tweet frequency by hour for one participant.
    Steps:
    1. Filter rows for given participant (by 'linker' ID).
    2. Count Tweets per hour (0-23 in 'tweet_hour').
    3. Store counts in a dictionary: {hour: tweet_count}.
    4. Fill missing hours with zero counts using missing_times().
    5. Convert dictionary to dataframe (one row: counts for each hour).
    6. Add participant ID as 'linker' column.
    7. Return hourly summary dataframe for the participant.
    """
    if id in data_f['linker'].unique():
        df_filter = data_f[data_f['linker'] == id]
        time_counts = df_filter.tweet_hour.value_counts()
        keys = np.array(time_counts.index)
        value = np.array(time_counts.values)
        diction = {keys[i]: value[i] for i in range(len(keys))}
        missing_times(diction, keys)
        df_times = pd.DataFrame.from_dict(diction, orient='index').transpose()
        df_times['linker'] = id
        return df_times

def missing_times(dic, keys):
    """
    Ensures all 24 time windows (hours 0 to 23) are present as dictionary keys.
    For hours with no Tweets, sets count to 0.
    """
    for n in range(24):
        if n not in keys:
            dic[n] = 0
    return dic

def data_gen(data_f, out_name):
    """
    Generates a wide-form dataset of Tweet timing features for each participant.
    Steps:
    1. List all unique participant IDs ('linker').
    2. For each ID, apply time_dataframe(), producing one hourly feature row.
    3. Concatenate all participant rows into a single dataframe.
    4. Sets output dataframe as global variable for use in analysis.
    Returns: Variable name of generated dataframe.
    """
    id_list = np.array(data_f['linker'].unique())
    frames = []
    for item in id_list:
        dt = time_dataframe(data_f, item)
        frames.append(dt)
    result = pd.concat(frames)
    var_name = out_name
    globals()[var_name] = result
    return var_name

def final_generation(df, name):
    """
    Merges participant-level Twitter and ALSPAC survey data, and generates new analytical measures.
    Key steps:
    - Creates full participant-hour wide table (using data_gen).
    - Computes total Tweet counts and row numbers.
    - Calculates proportions (e.g. early morning Tweets, Retweet ratio).
    - Computes mean LIWC NegEmo score per participant.
    - Generates unique participant and generation labels for analysis.
    - Saves the final dataset as CSV file for further use.
    """
    data_gen(df, 'final_df')
    global final_df
    # Retain hourly Tweet columns and participant ID
    final_df = final_df[['linker'] + list(range(24))]
    
    # Total Tweets per participant
    final_df['total_tweets'] = final_df.loc[:, range(24)].sum(axis=1)
    
    # Row number for sorting/merging
    final_df['row_num'] = np.arange(len(final_df))
    
    # Merge into survey data
    link = final_df[['linker', 'total_tweets']]
    final_d = surv_data.merge(final_df)
    final_d = final_d.sort_values(by='row_num')
    
    # Analytical features
    final_d['Proportion Early Morning'] = (final_d[1] + final_d[2] + final_d[3]) / final_d['total_tweets']
    final_d['Proportion of Retweets'] = final_d['Number of Retweets'] / final_d['total_tweets']
    
    # Compute mean negemo score
    df['liwcnegemo'] = df['liwcnegemo'].fillna(0)
    neg_df = df.groupby('linker')['liwcnegemo'].mean().reset_index()
    neg_df.rename(columns={'liwcnegemo': 'Mean Negemo Score'}, inplace=True)
    final_d = pd.merge(final_d, neg_df)
    
    # Create participant label variables
    final_d['aln'] = final_d['aln'].astype(str)
    final_d['linker'] = final_d['aln'] + ' ' + final_d['qlet']
    
    # Assign generation cohort label
    final_d['generation'] = None
    final_d.loc[final_d['qlet'].isin(['A', 'B']), 'generation'] = 'G1'
    final_d.loc[final_d['qlet'].isin(['P', 'M']), 'generation'] = 'G0'
    
    print(final_d)
    final_d.to_csv(f'C:/Users/ta20395/TD/Second Harvest Data/{name}.csv')

# -----------------------------------------------
# Main Procedure
# -----------------------------------------------
# Run full participant-level Twitter metric extraction and save results:
id_list = np.array(tl_df['linker'].unique())
final_generation(tl_df, 'NAME')

# To read output file for further analysis:
final_df = pd.read_csv("FILEPATH/NAME")
