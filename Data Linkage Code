#The purpose of this code is to link the ALSPAC survey data to the Tweet dataset.
#This generates a dataset where each row is a single participant, with their survey responses and aggregated Twitter data.
#In this example code, the survey data is surv_data, and the Tweet data is tweet_data
#All ALSPAC provided variable names have been replaced with new names. 
#We have not provided the code for the first linkage stages (i.e. cleaning data, generating the whole ALSPAC dataset) as these contain information 
#about the specific ALSPAC variable names used in our analysis. However these stages are described in detail below. 

#STEP 1: Importing Packages

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter
from sklearn.preprocessing import StandardScaler
from datetime import datetime
from datetime import timedelta
from matplotlib.dates import YearLocator, DateFormatter
import numpy as np
import math
import random
import time
import plotly.express as px
import plotly.graph_objects as go
from itertools import chain
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from dateutil.relativedelta import relativedelta

#STEP 2: Data Cleaning
#Prior to using this code, there are several data cleaning stages:
#1). Remove all participants who posted zero Tweets, yet are still in tweet_data
#2). In both surv_data and tweet_data, there is a variable which provides a unique code for each ALSPAC family (e.g. G0 and G1 share the same code),
#and a variable which captures which generation the participant was in. These must be combined together to generate a unique indentifier for each participant,
#which we called "linker". 

#STEP 3: GENERATING THE WHOLE ALSPAC COHORT
#Surv_data is currently in the format where each row is a family, and not an individual participant. 
#We need surv_data to be at the individual participant-level, as this is the basis for our analysis.
#To do this, produce three identical versions of surv_data, and then remove all but one of the ALSPAC particpant types.
#This will lead to individual datasets for mothers, partners and G1 children. 
#Within these dataset you must run the relevant ALSPAC filters, to ensure that all used participants are still enrolled in the cohort. 
#Produce a label variable that flags which dataset an observation is from (e.g. all mother dataset participants are given the label 'M').
#Then you can merge all the three datasets together, making a large participant-level dataset.

#STEP 4: Restricting survey data to participants who provided Twitter data
#Generate a variable which contains all the unique linker identifiers which are shared between surv_data and tweet_data

sd_link = list(surv_data['linker'])
td_link = list(tweet_data['linker'])
common_link = set(sd_link).intersection(td_link)
len(common_link) #check number of eligible participants

#Restrict surv_data to only participants who shared Twitter data
surv_filt = surv[surv['linker'].isin(td_link)]

#STEP 5: Adding participant-level Tweet metrics to surv_filt
#We want to produce a dataset, where each row is a participant, their survey data, and information about the Tweets they posted over a particular period of time
#Firstly, we have to merge surv_filt, into the tweet_data. We will refer to this new dataset as tl_df.

tl_df = tweet_data.merge(surv_filt)

#At this point, you can apply any filter (e.g. dates, type of Tweet, content of Tweet) to tl_df.

#Below are several functions which take tl_df, and convert it into the participant-level dataset we desire.
#These functions were initially designed for use in Chapter 4, hence the focus on hours of Tweet posting. 
#They were subsequently adapted to provide the variables needed in future chapters. 

"""
This function generates a dataframe containg timing of Tweet information for each participant. Several steps:
1). Go through each unique ID in tl_df, to see if it matches a list of IDs provided to the function 
2). For each ID, generate a dataframe only containing Tweets from this individual
3). Generate a count of how much they Tweet in each time window
4). Generates a dictionary from this count, the dictionary index is the time window and key is the number of Tweets
in that time window
5). Apply the missing times function to this dictionary, adding at empty column for time windows a participant posted no Tweets
in
6). Generate a dataframe from this dictionary 
7). Add ID column to the new dataframe
8). Results in each participant having a dataframe with columns storing information about how many Tweets they posted
in each time window
"""

def time_dataframe(data_f, id):
    if id in data_f['linker'].unique():
        is_id = data_f['linker'] == id
        df_filter = data_f[is_id] # Filters dataset by row

        time_counts = df_filter.tweet_hour.value_counts() # Generates count of each time window

        keys = np.array(time_counts.index)
        value = np.array(time_counts.values)
        diction = {keys[i]: value[i] for i in range(len(keys))} # Generates dictionary from time window data

        missing_times(diction, keys) # If dictionary key is missing (i.e. no Tweets during this time), add key with value 0

        df_times = pd.DataFrame.from_dict(diction, orient='index')
        df_times = df_times.transpose() # Create dataframe from this dictionary

        df_times['linker'] = id # Add ID column to dataframe
        return df_times

"""
When the merged dataset is created, if a participant did not Tweet at all in a particular time window, they will not have a
column for this time window. The function below adds a column in the dataframe for these participants, with a value of 0, to
ensure that there are no missing columns
"""

def missing_times(dic, keys):
    for n in range(0,24):
        if n not in keys:
            dic.update({n: 0})
    return dic

"""
This function uses the time_dataframe function to generate a dataset containing participant level Twitter behaviour measures
merged to the questionnaire responses. Several steps:
1). Generates an list of IDs, from the unique IDs in the dataframe
2). For every ID in the list, uses the time_dataframe function to generate a dataframe for this ID
3). Merges the dataframes for each participant ID together, to generate one big dataframe for all participants
4). Returns the dataframe as a variable named after the name the participant provided when calling the function
"""

def data_gen(data_f, out_name):
    id_list = np.array(data_f['linker'].unique())
    frames = []
    
    for item in id_list:
        dt = time_dataframe(data_f, item)
        frames.append(dt)

    result = pd.concat(frames)

    var_name = out_name

    str = var_name

    globals()[str] = result

    return var_name

#This function uses the functions above to generate the participant-level dataset
#This code can be adapt to provide a wide range of participant-level Tweet metrics. As an example, we have shown code used to generate:
#1). The proportion of Tweets posted at 1am to 3am, 2). the proportion of Retweets a participant posted, and 3). The mean LIWC NegEmo score in their Tweets

def final_generation(df, name):
    
    data_gen(df, 'final_df')
    global final_df
    final_df = final_df[['linker',0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]]

    # Generating total tweets variables
    final_df['total_tweets'] = final_df[0] + final_df[1] + final_df[2] + final_df[3] + final_df[4] + final_df[5] + final_df[6] + final_df[7] + final_df[8] + + final_df[9] + + final_df[10] + final_df[11]+ final_df[12]+ final_df[13]+ final_df[14]+ final_df[15]+ final_df[16]+ final_df[17]+ final_df[18]+ final_df[19]+ final_df[20]+ final_df[21] + final_df[22] + final_df[23]  

    #Generating row_num column, referring the row number of each of the dataframe's row
    final_df['row_num'] = np.arange(len(final_df))
    final_df['row_num']

    #Merging participant level Twitter dataset to ALSPAC survey dataset
    link = final_df[['linker', 'total_tweets']]
    final_d = surv.merge(final_df)
    final_d = final_d.sort_values(by=['row_num'])

    final_data['Proportion Early Morning'] = (final_d[1] + final_d[2] + final_d[3]) / final_d['total_tweets']
    
    final_d['Proportion of Retweets'] = final_d['Number of Retweets'] / final_d['total_tweets']
    
    df['liwcnegemo'] = df['liwcnegemo'].fillna(0)
    neg_df = df.groupby('linker')['liwcnegemo'].mean().reset_index()
    neg_df.rename(columns = {'liwcnegemo': 'Mean Negemo Score'}, inplace=True)
    final_d = pd.merge(final_d, neg_df)
    
    final_d['aln'] = final_d['aln'].astype(str)
    final_d['linker'] = final_d['aln'] + ' ' + final_d['qlet']
    
    final_d['generation'] = None
    final_d.loc[(final_d['qlet'] == 'A') | (final_d['qlet'] == 'B'), 'generation'] = 'G1'
    final_d.loc[(final_d['qlet'] == 'P') | (final_d['qlet'] == 'M'), 'generation'] = 'G0'

    
    print(final_d)

    final_d.to_csv(f'C:\\Users\\ta20395\\TD\\Second Harvest Data\\{name}.csv')

#The functions above all come together, and are run with this code:
id_list = np.array(pl_df['linker'].unique())
final_generation(tl_df, 'NAME')

#The generation variable is produced and saved to your chosen filepath, and can then be read into Python with:
final_df = pd.read_csv("FILEPATH/NAME")
